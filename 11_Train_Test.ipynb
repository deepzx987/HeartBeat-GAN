{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprising-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# example of training an conditional gan on the fashion mnist dataset\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "import keras\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, multiply, Embedding, merge, Concatenate, Conv1D, BatchNormalization\n",
    "from keras.layers import Dense, Flatten, Multiply, LSTM\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import UpSampling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Add\n",
    "import tensorflow as tf\n",
    "from evaluation_metrics import *\n",
    "metric_to_calculate = ['FID', 'MMD', 'DTW', 'PC', 'RMSE', 'TWED']\n",
    "from helper import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infinite-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 186\n",
    "INPUT_FEAT = 1\n",
    "OUTPUT_CLASS = 3\n",
    "BS = 256\n",
    "EP = 100\n",
    "Pat = 6\n",
    "\n",
    "# Mon_Term = 'val_AUC'\n",
    "# Mode = 'max'\n",
    "\n",
    "Mon_Term = 'val_loss'\n",
    "Mode = 'min'\n",
    "\n",
    "Model_Name = 'GAP_'\n",
    "def load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS):\n",
    "    return Parallel_GAP(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "\n",
    "# results = 'LSTM_'\n",
    "# def load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS):\n",
    "#     return parallel_NN_LSTM(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "\n",
    "results = 'Class_Results_' + Model_Name + Mon_Term\n",
    "if not os.path.exists(str(results)):\n",
    "    os.makedirs(str(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "important-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(X):\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "        return X\n",
    "    else:\n",
    "        if X.shape[-1] == 1:\n",
    "            return X\n",
    "        else:\n",
    "            X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "impossible-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163820, 186, 1) (40955, 186, 1) (163820,) (40955,)\n",
      "(array([0., 1., 2.]), array([125561,   9305,  28954]))\n",
      "(array([0., 1., 2.]), array([31181,  2358,  7416]))\n",
      "(203187, 186, 1) (203187,)\n",
      "(array([0., 1., 2.]), array([155008,  12564,  35615]))\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/TrainVal/X.npy'))\n",
    "y = np.load('Data/TrainVal/y.npy')\n",
    "X_val = reshape(np.load('Data/TrainVal/X_val.npy'))\n",
    "y_val = np.load('Data/TrainVal/y_val.npy')\n",
    "\n",
    "print (X.shape, X_val.shape, y.shape, y_val.shape)\n",
    "print (np.unique(y, return_counts=True))\n",
    "print (np.unique(y_val, return_counts=True))\n",
    "\n",
    "\n",
    "y = to_categorical(np.load('Data/TrainVal/y.npy'))\n",
    "y_val = to_categorical(np.load('Data/TrainVal/y_val.npy'))\n",
    "\n",
    "X_test = np.load('Data/Original/test_data.npy')\n",
    "y_test = X_test[:,-1]\n",
    "X_test = reshape(X_test[:,:-1])\n",
    "\n",
    "print (X_test.shape, y_test.shape)\n",
    "print (np.unique(y_test, return_counts=True))\n",
    "\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rural-premiere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# title = 'Original'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-mobility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manufactured-discretion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([125551, 125559, 125553]))\n",
      "(376663, 186, 1) (376663, 3)\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/SMOTETOMEK/X.npy'))\n",
    "y = np.load('Data/SMOTETOMEK/y.npy')\n",
    "print (np.unique(y, return_counts=True))\n",
    "y = to_categorical(y)\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "# title = 'SMOTETOMEK'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-semiconductor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([122941, 125410, 125193]))\n",
      "(373544, 186, 1) (373544, 3)\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/SMOTEENN/X.npy'))\n",
    "y = np.load('Data/SMOTEENN/y.npy')\n",
    "print (np.unique(y, return_counts=True))\n",
    "y = to_categorical(y)\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "# title = 'SMOTEENN'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "settled-joint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([125561, 125561, 125561]))\n",
      "(376683, 186, 1) (376683, 3)\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/SMOTE/X.npy'))\n",
    "y = np.load('Data/SMOTE/y.npy')\n",
    "print (np.unique(y, return_counts=True))\n",
    "y = to_categorical(y)\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "# title = 'SMOTE'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "certain-brooks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([125561, 125895, 125677]))\n",
      "(377133, 186, 1) (377133, 3)\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/ADASYN/X.npy'))\n",
    "y = np.load('Data/ADASYN/y.npy')\n",
    "print (np.unique(y, return_counts=True))\n",
    "y = to_categorical(y)\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "# title = 'ADASYN'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "above-approval",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.]), array([125561, 125561, 125561]))\n",
      "(376683, 186, 1) (376683, 3)\n"
     ]
    }
   ],
   "source": [
    "X = reshape(np.load('Data/SVMSMOTE/X.npy'))\n",
    "y = np.load('Data/SVMSMOTE/y.npy')\n",
    "print (np.unique(y, return_counts=True))\n",
    "y = to_categorical(y)\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "# title = 'SVMSMOTE'\n",
    "\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "\n",
    "# model = load_model_train(WINDOW_SIZE,INPUT_FEAT,OUTPUT_CLASS)\n",
    "# cb = [keras.callbacks.EarlyStopping(monitor=Mon_Term, patience=Pat, verbose=1, mode=Mode), \n",
    "#       ModelCheckpoint(filename+'/check.h5', save_best_only=True, monitor=Mon_Term, mode=Mode, verbose=1)]\n",
    "\n",
    "# history = model.fit(X, y, validation_data=(X_val, y_val), batch_size=BS, epochs=EP, callbacks=cb, verbose=1)\n",
    "# save_the_model(filename, model)\n",
    "# write_history(filename, history.history)\n",
    "# plt.figure(figsize=(5.5,3.2))\n",
    "# prediction(X_test, y_test, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-convergence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-gather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reduced-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Temp for bad results\n",
    "\n",
    "# def load_modell(filename):\n",
    "#     # load json and create model\n",
    "#     json_file = open(str(filename)+'/model.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     model.load_weights(str(filename)+\"/model.h5\")\n",
    "#     model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "#                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#                   metrics=[tf.keras.metrics.BinaryAccuracy(name='Accuracy'), \n",
    "#                            tf.keras.metrics.Recall(name='Recall'),  \n",
    "#                            tf.keras.metrics.Precision(name='Precision'), \n",
    "#                            tf.keras.metrics.AUC(num_thresholds=200, summation_method=\"interpolation\", \n",
    "#                                                 name=\"AUC\", dtype=None, curve=\"ROC\", thresholds=None, \n",
    "#                                                 multi_label=True, label_weights=None)])\n",
    "#     return model\n",
    "\n",
    "# title = 'Original'\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "    \n",
    "# model = load_modell(filename)\n",
    "# print ('Model Loaded')\n",
    "# y_pred=model.predict(X_test,verbose=0)\n",
    "\n",
    "# y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "# y_test_arg = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # y_pred=model.predict(X_test,verbose=1)\n",
    "# cm = ConfusionMatrix(actual_vector=y_test_arg, predict_vector=y_pred_arg)\n",
    "# totalt = cm.__dict__\n",
    "\n",
    "# TP = totalt['TP']\n",
    "# FP = totalt['FP']\n",
    "# TN = totalt['TN']\n",
    "# FN = totalt['FN']\n",
    "\n",
    "# PPV = totalt['PPV']\n",
    "# ACC = totalt['ACC']\n",
    "# SEN = totalt['TPR']\n",
    "# SPE = totalt['TNR']\n",
    "# F1S = totalt['F1']\n",
    "# AUC = totalt['AUC']\n",
    "\n",
    "# f = open(str(filename)+'/Results.csv', \"w\")\n",
    "# f.write('TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC\\n')\n",
    "# for i in range(3):\n",
    "#     f.write(str(TP[i])+','+str(FP[i])+','+str(TN[i])+','+str(FN[i])+','+str(PPV[i])+','+\n",
    "#             str(ACC[i])+','+str(SEN[i])+','+str(SPE[i])+','+str(F1S[i])+','+str(AUC[i])+'\\n')\n",
    "# f.close()\n",
    "\n",
    "# cnf_matrix = confusion_matrix(y_test_arg, y_pred_arg)\n",
    "# classes = ['Normal','SVEB','VEB']\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plot_confusion_matrix(cnf_matrix, classes, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structural-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# # SMOTE Temp for bad results\n",
    "\n",
    "# def load_modell(filename):\n",
    "#     # load json and create model\n",
    "#     json_file = open(str(filename)+'/model.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     model.load_weights(str(filename)+\"/model.h5\")\n",
    "#     model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "#                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#                   metrics=[tf.keras.metrics.BinaryAccuracy(name='Accuracy'), \n",
    "#                            tf.keras.metrics.Recall(name='Recall'),  \n",
    "#                            tf.keras.metrics.Precision(name='Precision'), \n",
    "#                            tf.keras.metrics.AUC(num_thresholds=200, summation_method=\"interpolation\", \n",
    "#                                                 name=\"AUC\", dtype=None, curve=\"ROC\", thresholds=None, \n",
    "#                                                 multi_label=True, label_weights=None)])\n",
    "#     return model\n",
    "\n",
    "# title = 'SMOTE'\n",
    "# filename = os.path.join(results,title)\n",
    "# if not os.path.exists(str(filename)):\n",
    "#     os.makedirs(str(filename))\n",
    "    \n",
    "# model = load_modell(filename)\n",
    "# print ('Model Loaded')\n",
    "# y_pred=model.predict(X_test,verbose=0)\n",
    "\n",
    "# y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "# y_test_arg = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # y_pred=model.predict(X_test,verbose=1)\n",
    "# cm = ConfusionMatrix(actual_vector=y_test_arg, predict_vector=y_pred_arg)\n",
    "# totalt = cm.__dict__\n",
    "\n",
    "# TP = totalt['TP']\n",
    "# FP = totalt['FP']\n",
    "# TN = totalt['TN']\n",
    "# FN = totalt['FN']\n",
    "\n",
    "# PPV = totalt['PPV']\n",
    "# ACC = totalt['ACC']\n",
    "# SEN = totalt['TPR']\n",
    "# SPE = totalt['TNR']\n",
    "# F1S = totalt['F1']\n",
    "# AUC = totalt['AUC']\n",
    "\n",
    "# f = open(str(filename)+'/Results.csv', \"w\")\n",
    "# f.write('TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC\\n')\n",
    "# for i in range(3):\n",
    "#     f.write(str(TP[i])+','+str(FP[i])+','+str(TN[i])+','+str(FN[i])+','+str(PPV[i])+','+\n",
    "#             str(ACC[i])+','+str(SEN[i])+','+str(SPE[i])+','+str(F1S[i])+','+str(AUC[i])+'\\n')\n",
    "# f.close()\n",
    "\n",
    "# cnf_matrix = confusion_matrix(y_test_arg, y_pred_arg)\n",
    "# classes = ['Normal','SVEB','VEB']\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plot_confusion_matrix(cnf_matrix, classes, filename, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-edition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
