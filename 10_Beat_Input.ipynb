{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprising-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# example of training an conditional gan on the fashion mnist dataset\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "import keras\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, multiply, Embedding, merge, Concatenate, Conv1D, BatchNormalization\n",
    "from keras.layers import Dense, Flatten, Multiply\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import UpSampling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Add\n",
    "import tensorflow as tf\n",
    "from evaluation_metrics import *\n",
    "from helper import *\n",
    "metric_to_calculate = ['FID', 'MMD', 'DTW', 'PC', 'RMSE', 'TWED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpine-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(data_dim=186, beat_dim=186):\n",
    "    \n",
    "    in_label = Input(shape=(beat_dim,1))\n",
    "    D_in = Input(shape=[data_dim,1])\n",
    "    inp1 = Concatenate()([D_in, in_label])\n",
    "\n",
    "    x = Conv1D(filters=48, kernel_size=19, padding='same', strides=4, kernel_initializer='he_normal')(inp1)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv1D(filters=64, kernel_size=15, padding='same', strides=3, kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv1D(filters=80, kernel_size=11, padding='same', strides=2, kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv1D(filters=96, kernel_size=9, padding='same', strides=2, kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv1D(filters=112, kernel_size=7, padding='same', strides=2, kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x1 = Conv1D(filters=48, kernel_size=9, padding='same', strides=4, kernel_initializer='he_normal')(inp1)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = Conv1D(filters=64, kernel_size=7, padding='same', strides=3, kernel_initializer='he_normal')(x1)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = Conv1D(filters=80, kernel_size=5, padding='same', strides=2, kernel_initializer='he_normal')(x1)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = Conv1D(filters=96, kernel_size=3, padding='same', strides=2, kernel_initializer='he_normal')(x1)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = Conv1D(filters=112, kernel_size=3, padding='same', strides=2, kernel_initializer='he_normal')(x1)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "    x1 = GlobalAveragePooling1D()(x1)\n",
    "\n",
    "    xx = concatenate([x,x1])\n",
    "\n",
    "    xx = Dense(100)(xx)\n",
    "    xx = Dense(100)(xx)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(xx)\n",
    "\n",
    "    model = Model(inputs=[D_in, in_label], outputs=out)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    loss = 'binary_crossentropy'\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "    # model.summary()\n",
    "\n",
    "# d_model = discriminator(data_dim=186, beat_dim=186)\n",
    "# plot_model(d_model, to_file='temp.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "northern-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_dim=186, beat_dim=186, out_dim=186):\n",
    "    \n",
    "    in_label = Input(shape=(beat_dim,1))\n",
    "    G_in = Input(shape=[noise_dim,1])\n",
    "    x = Concatenate()([G_in, in_label])\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*16, kernel_size=2, strides=2, padding='valid', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*8, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*8, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*4, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*4, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32*2, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=32, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(filters=1, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
    "    out = Activation('tanh')(x)\n",
    "    model = Model(inputs=[G_in, in_label], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "# g_model = generator(noise_dim=186, beat_dim=186, out_dim=186)\n",
    "# plot_model(g_model, to_file='temp.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "written-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(d_model, g_model):\n",
    "    \n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_beat = g_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = g_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = d_model([gen_output, gen_beat])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_beat], gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    loss = 'binary_crossentropy'\n",
    "    model.compile(loss=loss, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# gan_model = create_gan(d_model, g_model)\n",
    "# plot_model(gan_model, to_file='Final/gan.pdf', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "important-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(X):\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "        return X\n",
    "    else:\n",
    "        if X.shape[-1] == 1:\n",
    "            return X\n",
    "        else:\n",
    "            X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocal-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    X = np.load('Data/ForGAN/X.npy')\n",
    "    y = np.load('Data/ForGAN/y.npy')\n",
    "\n",
    "    # print (X.shape, y.shape)\n",
    "\n",
    "    X_N = X[y==0]\n",
    "    X_S = X[y==1]\n",
    "    X_V = X[y==2]\n",
    "\n",
    "    y_N = y[y==0]\n",
    "    y_S = y[y==1]\n",
    "    y_V = y[y==2]\n",
    "\n",
    "    return reshape(X_N), y_N, reshape(X_S), y_S, reshape(X_V), y_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "certified-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(X_N, y_N, X_S, y_S, X_V, y_V, n_samples):\n",
    "    \n",
    "    # choose random instances\n",
    "    i_N = randint(0, y_N.shape[0], int(n_samples/3))\n",
    "    i_S = randint(0, y_S.shape[0], int(n_samples/3))\n",
    "    i_V = randint(0, y_V.shape[0], int(n_samples/3))\n",
    "    \n",
    "    # select ECG and labels\n",
    "    X = np.vstack((X_N[i_N], X_S[i_S], X_V[i_V]))\n",
    "    labels = keras.utils.to_categorical(np.hstack((y_N[i_N], y_S[i_S], y_V[i_V])))\n",
    "    # print (labels.shape)\n",
    "    \n",
    "    # generate class labels\n",
    "    y = reshape(np.random.uniform(0.8, 1, n_samples))\n",
    "\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bound-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "# normal noise\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=3):\n",
    "    # generate points in the latent space\n",
    "#     X_fake = np.random.uniform(0, 1.0, size=[n_samples, latent_dim])\n",
    "    X_fake = np.random.normal(0,1.0,(n_samples,latent_dim))\n",
    "    # generate labels\n",
    "    labels_fake = np.hstack((np.zeros(int(n_samples/3)), np.ones(int(n_samples/3)), 2*np.ones(int(n_samples/3))))\n",
    "    np.random.shuffle(labels_fake)\n",
    "    return [reshape(X_fake), keras.utils.to_categorical(labels_fake)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solar-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    ecgs = generator.predict([z_input, z_input])\n",
    "    # create class labels\n",
    "    y = reshape(np.random.uniform(0, 0.2, n_samples))\n",
    "\n",
    "    return [ecgs, labels_input], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "general-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save a plot of generated images\n",
    "def save_plot(X, n):\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.plot(X[i, :, 0])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "communist-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_samples(X_N, y_N, X_S, y_S, X_V, y_V):\n",
    "    \n",
    "    # choose random instances\n",
    "    i_N = randint(0, y_N.shape[0], 1)\n",
    "    i_S = randint(0, y_S.shape[0], 1)\n",
    "    i_V = randint(0, y_V.shape[0], 1)\n",
    "    \n",
    "    # select ECG and labels\n",
    "    X = np.vstack((X_N[i_N], X_S[i_S], X_V[i_V]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "speaking-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_plot(X_R, z_input, n_batch, name):\n",
    "    n = 3\n",
    "    Win = (n_batch//3)\n",
    "    XX = np.vstack((X_R, z_input[0:n,:,:], z_input[Win:Win+n,:,:], z_input[2*Win:2*Win+n,:,:]))\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for i in range(n):\n",
    "        # subplot(R, C, Plot_No)\n",
    "        plt.subplot(n+1, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.plot(XX[i,:,:])\n",
    "    for i in range(n, ((n+1)*(n+1))-(n+1)):\n",
    "        # define subplot\n",
    "        plt.subplot(n+1, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.plot(XX[i,:,:])\n",
    "    # plt.show()\n",
    "    plt.savefig(name, dpi=75)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cubic-shirt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0, 1/80, d1=0.43123, d2=1.23404 g=0.41893\n",
      ">0, 3/80, d1=0.69851, d2=0.59896 g=1.11571\n",
      ">0, 5/80, d1=0.94723, d2=0.49260 g=0.99789\n",
      ">0, 7/80, d1=0.79130, d2=0.54421 g=0.90162\n",
      ">0, 9/80, d1=0.73900, d2=0.48653 g=0.82177\n",
      ">0, 11/80, d1=0.67116, d2=0.44193 g=0.68087\n",
      ">0, 13/80, d1=0.59109, d2=0.51439 g=0.68262\n",
      ">0, 15/80, d1=0.52059, d2=0.56807 g=0.63171\n",
      ">0, 17/80, d1=0.77645, d2=0.46083 g=1.08061\n",
      ">0, 19/80, d1=0.64086, d2=0.35679 g=0.62802\n",
      ">0, 21/80, d1=0.48687, d2=0.42539 g=0.59504\n",
      ">0, 23/80, d1=0.50331, d2=0.33484 g=0.48040\n",
      ">0, 25/80, d1=0.42029, d2=0.44335 g=0.49474\n",
      ">0, 27/80, d1=0.43937, d2=0.42185 g=0.42326\n",
      ">0, 29/80, d1=0.42809, d2=0.32842 g=0.36781\n",
      ">0, 31/80, d1=0.36859, d2=0.35872 g=0.36772\n",
      ">0, 33/80, d1=0.35481, d2=0.36613 g=0.35331\n",
      ">0, 35/80, d1=0.36385, d2=0.35404 g=0.35887\n",
      ">0, 37/80, d1=0.37847, d2=0.34437 g=0.34323\n",
      ">0, 39/80, d1=0.37146, d2=0.31501 g=0.33937\n",
      ">0, 41/80, d1=0.36941, d2=0.32617 g=0.34835\n",
      ">0, 43/80, d1=0.34892, d2=0.33522 g=0.33943\n",
      ">0, 45/80, d1=0.34539, d2=0.34801 g=0.33674\n",
      ">0, 47/80, d1=0.34234, d2=0.37424 g=0.46164\n",
      ">0, 49/80, d1=0.33274, d2=0.42479 g=0.33210\n",
      ">0, 51/80, d1=0.33653, d2=0.33184 g=0.33587\n",
      ">0, 53/80, d1=0.35642, d2=0.33987 g=0.34612\n",
      ">0, 55/80, d1=0.33255, d2=0.35976 g=0.34587\n",
      ">0, 57/80, d1=0.34809, d2=0.46906 g=0.47782\n",
      ">0, 59/80, d1=0.34049, d2=0.41050 g=0.35182\n",
      ">0, 61/80, d1=0.35198, d2=0.38177 g=0.34441\n",
      ">0, 63/80, d1=0.35521, d2=0.36462 g=0.33755\n",
      ">0, 65/80, d1=0.33324, d2=0.33504 g=0.32985\n",
      ">0, 67/80, d1=0.34382, d2=0.32941 g=0.33483\n",
      ">0, 69/80, d1=0.34117, d2=0.35335 g=0.33334\n",
      ">0, 71/80, d1=0.35012, d2=0.32631 g=0.33385\n",
      ">0, 73/80, d1=0.35046, d2=0.32897 g=0.33938\n",
      ">0, 75/80, d1=0.33552, d2=0.34265 g=0.33696\n",
      ">0, 77/80, d1=0.33461, d2=0.32836 g=0.33901\n",
      ">0, 79/80, d1=0.33416, d2=0.35928 g=0.33078\n",
      ">1, 1/80, d1=0.35688, d2=0.33487 g=0.34239\n",
      ">1, 3/80, d1=0.32342, d2=0.33528 g=0.33949\n",
      ">1, 5/80, d1=0.34401, d2=0.34057 g=0.32889\n",
      ">1, 7/80, d1=0.34365, d2=0.34812 g=0.34820\n",
      ">1, 9/80, d1=0.33761, d2=0.33337 g=0.32938\n",
      ">1, 11/80, d1=0.33585, d2=0.32925 g=0.33793\n",
      ">1, 13/80, d1=0.33853, d2=0.33303 g=0.33903\n",
      ">1, 15/80, d1=0.34128, d2=0.32722 g=0.33821\n",
      ">1, 17/80, d1=0.34643, d2=0.34805 g=0.32916\n",
      ">1, 19/80, d1=0.33681, d2=0.33759 g=0.32622\n",
      ">1, 21/80, d1=0.33091, d2=0.34826 g=0.33013\n",
      ">1, 23/80, d1=0.35256, d2=0.34767 g=0.33673\n",
      ">1, 25/80, d1=0.35021, d2=0.33519 g=0.34388\n",
      ">1, 27/80, d1=0.32796, d2=0.32347 g=0.32674\n",
      ">1, 29/80, d1=0.33100, d2=0.33391 g=0.33035\n",
      ">1, 31/80, d1=0.34739, d2=0.33174 g=0.33927\n",
      ">1, 33/80, d1=0.32014, d2=0.32864 g=0.31877\n",
      ">1, 35/80, d1=0.32917, d2=0.34604 g=0.32845\n",
      ">1, 37/80, d1=0.36210, d2=0.33764 g=0.33038\n",
      ">1, 39/80, d1=0.31236, d2=0.32480 g=0.33412\n",
      ">1, 41/80, d1=0.32257, d2=0.34044 g=0.32733\n",
      ">1, 43/80, d1=0.34035, d2=0.34517 g=0.31637\n",
      ">1, 45/80, d1=0.34513, d2=0.32331 g=0.34269\n",
      ">1, 47/80, d1=0.33607, d2=0.33834 g=0.33640\n",
      ">1, 49/80, d1=0.33898, d2=0.32870 g=0.32854\n",
      ">1, 51/80, d1=0.33363, d2=0.32251 g=0.32995\n",
      ">1, 53/80, d1=0.33419, d2=0.33449 g=0.32947\n",
      ">1, 55/80, d1=0.31715, d2=0.32710 g=0.33981\n",
      ">1, 57/80, d1=0.32369, d2=0.35217 g=0.32577\n",
      ">1, 59/80, d1=0.34421, d2=0.33949 g=0.32884\n",
      ">1, 61/80, d1=0.33495, d2=0.32200 g=0.33656\n",
      ">1, 63/80, d1=0.31589, d2=0.34026 g=0.33635\n",
      ">1, 65/80, d1=0.33703, d2=0.33998 g=0.33999\n",
      ">1, 67/80, d1=0.34195, d2=0.34927 g=0.33961\n",
      ">1, 69/80, d1=0.33604, d2=0.35414 g=0.32012\n",
      ">1, 71/80, d1=0.32291, d2=0.33547 g=0.32182\n",
      ">1, 73/80, d1=0.33023, d2=0.32804 g=0.32352\n",
      ">1, 75/80, d1=0.34043, d2=0.32504 g=0.32986\n",
      ">1, 77/80, d1=0.32649, d2=0.32602 g=0.33024\n",
      ">1, 79/80, d1=0.33489, d2=0.33122 g=0.33006\n",
      ">2, 1/80, d1=0.33597, d2=0.32173 g=0.32375\n",
      ">2, 3/80, d1=0.32253, d2=0.34297 g=0.32670\n",
      ">2, 5/80, d1=0.33597, d2=0.32975 g=0.32391\n",
      ">2, 7/80, d1=0.34514, d2=0.33931 g=0.31875\n",
      ">2, 9/80, d1=0.35105, d2=0.32618 g=0.33756\n",
      ">2, 11/80, d1=0.35042, d2=0.32533 g=0.32352\n",
      ">2, 13/80, d1=0.33036, d2=0.33309 g=0.32276\n",
      ">2, 15/80, d1=0.32251, d2=0.34627 g=0.31943\n",
      ">2, 17/80, d1=0.34132, d2=0.33907 g=0.33952\n",
      ">2, 19/80, d1=0.34171, d2=0.31000 g=0.33628\n",
      ">2, 21/80, d1=0.32773, d2=0.33172 g=0.34036\n",
      ">2, 23/80, d1=0.32791, d2=0.29935 g=0.32661\n",
      ">2, 25/80, d1=0.32709, d2=0.33102 g=0.32974\n",
      ">2, 27/80, d1=0.34408, d2=0.33288 g=0.32736\n",
      ">2, 29/80, d1=0.34332, d2=0.32350 g=0.33787\n",
      ">2, 31/80, d1=0.34289, d2=0.34562 g=0.32555\n",
      ">2, 33/80, d1=0.33192, d2=0.32493 g=0.33829\n",
      ">2, 35/80, d1=0.33299, d2=0.34919 g=0.32632\n",
      ">2, 37/80, d1=0.33188, d2=0.32761 g=0.32280\n",
      ">2, 39/80, d1=0.33501, d2=0.32957 g=0.32769\n",
      ">2, 41/80, d1=0.33052, d2=0.33670 g=0.32075\n",
      ">2, 43/80, d1=0.34341, d2=0.32129 g=0.33942\n",
      ">2, 45/80, d1=0.34714, d2=0.33666 g=0.34847\n",
      ">2, 47/80, d1=0.34249, d2=0.34791 g=0.33941\n",
      ">2, 49/80, d1=0.32499, d2=0.32685 g=0.33247\n",
      ">2, 51/80, d1=0.32483, d2=0.32153 g=0.33234\n",
      ">2, 53/80, d1=0.33687, d2=0.31574 g=0.33446\n",
      ">2, 55/80, d1=0.32549, d2=0.34132 g=0.32707\n",
      ">2, 57/80, d1=0.31560, d2=0.33899 g=0.32147\n",
      ">2, 59/80, d1=0.31522, d2=0.32213 g=0.32660\n",
      ">2, 61/80, d1=0.33245, d2=0.32957 g=0.33253\n",
      ">2, 63/80, d1=0.31586, d2=0.33584 g=0.33662\n",
      ">2, 65/80, d1=0.34360, d2=0.33302 g=0.33224\n",
      ">2, 67/80, d1=0.32465, d2=0.34011 g=0.31813\n",
      ">2, 69/80, d1=0.33785, d2=0.33852 g=0.31428\n",
      ">2, 71/80, d1=0.34429, d2=0.31579 g=0.33526\n",
      ">2, 73/80, d1=0.32322, d2=0.32636 g=0.33903\n",
      ">2, 75/80, d1=0.34208, d2=0.33808 g=0.32686\n",
      ">2, 77/80, d1=0.33277, d2=0.34890 g=0.31816\n",
      ">2, 79/80, d1=0.33497, d2=0.32627 g=0.33150\n",
      ">3, 1/80, d1=0.33127, d2=0.32842 g=0.31793\n",
      ">3, 3/80, d1=0.31617, d2=0.32815 g=0.33464\n",
      ">3, 5/80, d1=0.32726, d2=0.31545 g=0.32117\n",
      ">3, 7/80, d1=0.33828, d2=0.31497 g=0.31357\n",
      ">3, 9/80, d1=0.34044, d2=0.32227 g=0.32856\n",
      ">3, 11/80, d1=0.33106, d2=0.33573 g=0.32976\n",
      ">3, 13/80, d1=0.34696, d2=0.32028 g=0.33051\n",
      ">3, 15/80, d1=0.33196, d2=0.33002 g=0.31188\n",
      ">3, 17/80, d1=0.32838, d2=0.32866 g=0.34015\n",
      ">3, 19/80, d1=0.32908, d2=0.33898 g=0.33251\n",
      ">3, 21/80, d1=0.33570, d2=0.31001 g=0.33063\n",
      ">3, 23/80, d1=0.34266, d2=0.31785 g=0.33604\n",
      ">3, 25/80, d1=0.32969, d2=0.32646 g=0.33090\n",
      ">3, 27/80, d1=0.31845, d2=0.33118 g=0.34288\n",
      ">3, 29/80, d1=0.33749, d2=0.32145 g=0.34376\n",
      ">3, 31/80, d1=0.33732, d2=0.32775 g=0.34689\n",
      ">3, 33/80, d1=0.34190, d2=0.31617 g=0.32856\n",
      ">3, 35/80, d1=0.32391, d2=0.32291 g=0.32923\n",
      ">3, 37/80, d1=0.32438, d2=0.33297 g=0.32139\n",
      ">3, 39/80, d1=0.33205, d2=0.31772 g=0.33298\n",
      ">3, 41/80, d1=0.34110, d2=0.32482 g=0.32931\n",
      ">3, 43/80, d1=0.32555, d2=0.32940 g=0.33003\n",
      ">3, 45/80, d1=0.34680, d2=0.34260 g=0.31973\n",
      ">3, 47/80, d1=0.34632, d2=0.32831 g=0.34085\n",
      ">3, 49/80, d1=0.33369, d2=0.34402 g=0.32965\n",
      ">3, 51/80, d1=0.32452, d2=0.32172 g=0.33819\n",
      ">3, 53/80, d1=0.31224, d2=0.32274 g=0.34093\n",
      ">3, 55/80, d1=0.32750, d2=0.30435 g=0.34264\n",
      ">3, 57/80, d1=0.33136, d2=0.33095 g=0.32366\n",
      ">3, 59/80, d1=0.32641, d2=0.31703 g=0.32631\n",
      ">3, 61/80, d1=0.33126, d2=0.33727 g=0.33082\n",
      ">3, 63/80, d1=0.34363, d2=0.33810 g=0.33034\n",
      ">3, 65/80, d1=0.34166, d2=0.33223 g=0.32158\n",
      ">3, 67/80, d1=0.32494, d2=0.33653 g=0.32128\n",
      ">3, 69/80, d1=0.34083, d2=0.33513 g=0.32290\n",
      ">3, 71/80, d1=0.31931, d2=0.33027 g=0.33678\n",
      ">3, 73/80, d1=0.31999, d2=0.30722 g=0.32738\n",
      ">3, 75/80, d1=0.31220, d2=0.33768 g=0.32188\n",
      ">3, 77/80, d1=0.31617, d2=0.34547 g=0.32471\n",
      ">3, 79/80, d1=0.33028, d2=0.32967 g=0.33401\n",
      ">4, 1/80, d1=0.33861, d2=0.34054 g=0.32844\n",
      ">4, 3/80, d1=0.33573, d2=0.31772 g=0.34332\n",
      ">4, 5/80, d1=0.33272, d2=0.33436 g=0.32376\n",
      ">4, 7/80, d1=0.32840, d2=0.30500 g=0.31892\n",
      ">4, 9/80, d1=0.31290, d2=0.32746 g=0.32423\n",
      ">4, 11/80, d1=0.33099, d2=0.33355 g=0.33160\n",
      ">4, 13/80, d1=0.32125, d2=0.32014 g=0.33629\n",
      ">4, 15/80, d1=0.32346, d2=0.32971 g=0.31210\n",
      ">4, 17/80, d1=0.33566, d2=0.32810 g=0.31978\n",
      ">4, 19/80, d1=0.33297, d2=0.35487 g=0.31771\n",
      ">4, 21/80, d1=0.32658, d2=0.34896 g=0.34897\n",
      ">4, 23/80, d1=0.34094, d2=0.33596 g=0.32951\n",
      ">4, 25/80, d1=0.32944, d2=0.33950 g=0.32595\n",
      ">4, 27/80, d1=0.31676, d2=0.35053 g=0.33807\n",
      ">4, 29/80, d1=0.33205, d2=0.32362 g=0.33334\n",
      ">4, 31/80, d1=0.34346, d2=0.32408 g=0.32966\n",
      ">4, 33/80, d1=0.35939, d2=0.33260 g=0.32996\n",
      ">4, 35/80, d1=0.32488, d2=0.34947 g=0.32003\n",
      ">4, 37/80, d1=0.32727, d2=0.32590 g=0.32492\n",
      ">4, 39/80, d1=0.33063, d2=0.34139 g=0.32680\n",
      ">4, 41/80, d1=0.33291, d2=0.33503 g=0.32899\n",
      ">4, 43/80, d1=0.35886, d2=0.34178 g=0.33887\n",
      ">4, 45/80, d1=0.33890, d2=0.33159 g=0.34592\n",
      ">4, 47/80, d1=0.33596, d2=0.32096 g=0.32950\n",
      ">4, 49/80, d1=0.33564, d2=0.34097 g=0.33717\n",
      ">4, 51/80, d1=0.33553, d2=0.31896 g=0.33011\n",
      ">4, 53/80, d1=0.33144, d2=0.32398 g=0.32753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">4, 55/80, d1=0.30647, d2=0.33085 g=0.32188\n",
      ">4, 57/80, d1=0.33509, d2=0.33153 g=0.32269\n",
      ">4, 59/80, d1=0.31878, d2=0.33345 g=0.32600\n",
      ">4, 61/80, d1=0.32501, d2=0.33142 g=0.32720\n",
      ">4, 63/80, d1=0.32315, d2=0.32092 g=0.32733\n",
      ">4, 65/80, d1=0.32472, d2=0.31568 g=0.33971\n",
      ">4, 67/80, d1=0.32719, d2=0.32049 g=0.32436\n",
      ">4, 69/80, d1=0.33005, d2=0.31893 g=0.32689\n",
      ">4, 71/80, d1=0.31952, d2=0.33341 g=0.32703\n",
      ">4, 73/80, d1=0.32502, d2=0.30740 g=0.32948\n",
      ">4, 75/80, d1=0.32469, d2=0.33086 g=0.33783\n",
      ">4, 77/80, d1=0.32903, d2=0.33522 g=0.32573\n",
      ">4, 79/80, d1=0.33981, d2=0.31432 g=0.32237\n",
      ">5, 1/80, d1=0.32828, d2=0.32929 g=0.32673\n",
      ">5, 3/80, d1=0.30502, d2=0.32308 g=0.32168\n",
      ">5, 5/80, d1=0.32429, d2=0.32926 g=0.33990\n",
      ">5, 7/80, d1=0.32241, d2=0.32445 g=0.33178\n",
      ">5, 9/80, d1=0.33795, d2=0.31703 g=0.32579\n",
      ">5, 11/80, d1=0.33256, d2=0.33801 g=0.32528\n",
      ">5, 13/80, d1=0.32250, d2=0.34113 g=0.32722\n",
      ">5, 15/80, d1=0.34257, d2=0.32921 g=0.34118\n",
      ">5, 17/80, d1=0.31522, d2=0.31932 g=0.31928\n",
      ">5, 19/80, d1=0.30972, d2=0.33651 g=0.33217\n",
      ">5, 21/80, d1=0.33390, d2=0.32288 g=0.32883\n",
      ">5, 23/80, d1=0.33256, d2=0.34138 g=0.33670\n",
      ">5, 25/80, d1=0.32488, d2=0.33297 g=0.33241\n",
      ">5, 27/80, d1=0.33214, d2=0.33138 g=0.32245\n",
      ">5, 29/80, d1=0.34086, d2=0.34658 g=0.33088\n",
      ">5, 31/80, d1=0.33216, d2=0.31092 g=0.33882\n",
      ">5, 33/80, d1=0.32827, d2=0.32132 g=0.33242\n",
      ">5, 35/80, d1=0.33424, d2=0.32273 g=0.30655\n",
      ">5, 37/80, d1=0.31471, d2=0.32675 g=0.34725\n",
      ">5, 39/80, d1=0.33055, d2=0.33868 g=0.32361\n",
      ">5, 41/80, d1=0.32122, d2=0.32877 g=0.32441\n",
      ">5, 43/80, d1=0.32719, d2=0.34055 g=0.31821\n",
      ">5, 45/80, d1=0.33859, d2=0.33543 g=0.33188\n",
      ">5, 47/80, d1=0.31785, d2=0.33744 g=0.32318\n",
      ">5, 49/80, d1=0.32501, d2=0.33066 g=0.34288\n",
      ">5, 51/80, d1=0.31863, d2=0.31026 g=0.33744\n",
      ">5, 53/80, d1=0.32922, d2=0.33697 g=0.31885\n",
      ">5, 55/80, d1=0.32977, d2=0.32943 g=0.32931\n",
      ">5, 57/80, d1=0.32492, d2=0.32814 g=0.32348\n",
      ">5, 59/80, d1=0.32295, d2=0.32866 g=0.33439\n",
      ">5, 61/80, d1=0.33133, d2=0.32657 g=0.32613\n",
      ">5, 63/80, d1=0.34126, d2=0.33732 g=0.32268\n",
      ">5, 65/80, d1=0.31483, d2=0.32789 g=0.32850\n",
      ">5, 67/80, d1=0.31970, d2=0.33607 g=0.31257\n",
      ">5, 69/80, d1=0.34430, d2=0.33553 g=0.32198\n",
      ">5, 71/80, d1=0.34428, d2=0.33978 g=0.33363\n",
      ">5, 73/80, d1=0.33698, d2=0.34654 g=0.34182\n",
      ">5, 75/80, d1=0.32556, d2=0.33314 g=0.33974\n",
      ">5, 77/80, d1=0.32451, d2=0.33283 g=0.33059\n",
      ">5, 79/80, d1=0.32622, d2=0.33053 g=0.31698\n",
      ">6, 1/80, d1=0.31879, d2=0.33869 g=0.34530\n",
      ">6, 3/80, d1=0.33907, d2=0.33666 g=0.33321\n",
      ">6, 5/80, d1=0.32493, d2=0.31046 g=0.31563\n",
      ">6, 7/80, d1=0.32467, d2=0.31709 g=0.33581\n",
      ">6, 9/80, d1=0.32651, d2=0.33810 g=0.32245\n",
      ">6, 11/80, d1=0.32819, d2=0.33806 g=0.33455\n",
      ">6, 13/80, d1=0.33029, d2=0.31961 g=0.32027\n",
      ">6, 15/80, d1=0.32123, d2=0.33197 g=0.32765\n",
      ">6, 17/80, d1=0.32010, d2=0.31888 g=0.33387\n",
      ">6, 19/80, d1=0.32683, d2=0.33197 g=0.33594\n",
      ">6, 21/80, d1=0.32123, d2=0.33280 g=0.33308\n",
      ">6, 23/80, d1=0.31968, d2=0.33019 g=0.33960\n",
      ">6, 25/80, d1=0.35630, d2=0.33082 g=0.35377\n",
      ">6, 27/80, d1=0.35970, d2=0.32500 g=0.34019\n",
      ">6, 29/80, d1=0.32249, d2=0.32706 g=0.34333\n",
      ">6, 31/80, d1=0.33377, d2=0.34882 g=0.33433\n",
      ">6, 33/80, d1=0.31314, d2=0.35335 g=0.32246\n",
      ">6, 35/80, d1=0.33673, d2=0.30899 g=0.32048\n",
      ">6, 37/80, d1=0.34363, d2=0.32538 g=0.32268\n",
      ">6, 39/80, d1=0.32998, d2=0.32732 g=0.32941\n",
      ">6, 41/80, d1=0.32104, d2=0.34164 g=0.33983\n",
      ">6, 43/80, d1=0.31950, d2=0.31765 g=0.33005\n",
      ">6, 45/80, d1=0.32480, d2=0.31662 g=0.32327\n",
      ">6, 47/80, d1=0.33707, d2=0.33694 g=0.32737\n",
      ">6, 49/80, d1=0.33973, d2=0.30941 g=0.33218\n",
      ">6, 51/80, d1=0.33613, d2=0.34593 g=0.33856\n",
      ">6, 53/80, d1=0.34125, d2=0.33664 g=0.31825\n",
      ">6, 55/80, d1=0.31204, d2=0.32567 g=0.33413\n",
      ">6, 57/80, d1=0.32001, d2=0.32946 g=0.33945\n",
      ">6, 59/80, d1=0.33034, d2=0.31877 g=0.31671\n",
      ">6, 61/80, d1=0.34116, d2=0.32957 g=0.32838\n",
      ">6, 63/80, d1=0.34384, d2=0.32522 g=0.32779\n",
      ">6, 65/80, d1=0.29337, d2=0.31537 g=0.32720\n",
      ">6, 67/80, d1=0.34388, d2=0.35607 g=0.31678\n",
      ">6, 69/80, d1=0.31836, d2=0.31285 g=0.34130\n",
      ">6, 71/80, d1=0.33560, d2=0.34084 g=0.33300\n",
      ">6, 73/80, d1=0.31802, d2=0.32121 g=0.33482\n",
      ">6, 75/80, d1=0.32361, d2=0.32546 g=0.31605\n",
      ">6, 77/80, d1=0.33093, d2=0.31806 g=0.32561\n",
      ">6, 79/80, d1=0.31585, d2=0.35493 g=0.32344\n",
      ">7, 1/80, d1=0.33858, d2=0.33743 g=0.33476\n",
      ">7, 3/80, d1=0.33747, d2=0.32584 g=0.31899\n",
      ">7, 5/80, d1=0.32738, d2=0.33459 g=0.33078\n",
      ">7, 7/80, d1=0.33889, d2=0.32822 g=0.32543\n",
      ">7, 9/80, d1=0.33600, d2=0.34394 g=0.33396\n",
      ">7, 11/80, d1=0.33143, d2=0.33323 g=0.33076\n",
      ">7, 13/80, d1=0.34367, d2=0.32993 g=0.31892\n",
      ">7, 15/80, d1=0.33412, d2=0.33639 g=0.32074\n",
      ">7, 17/80, d1=0.33114, d2=0.32811 g=0.32236\n",
      ">7, 19/80, d1=0.33125, d2=0.31089 g=0.32481\n",
      ">7, 21/80, d1=0.34304, d2=0.32652 g=0.32740\n",
      ">7, 23/80, d1=0.31738, d2=0.34409 g=0.32628\n",
      ">7, 25/80, d1=0.33819, d2=0.34529 g=0.32992\n",
      ">7, 27/80, d1=0.32963, d2=0.33782 g=0.33498\n",
      ">7, 29/80, d1=0.31987, d2=0.31176 g=0.33779\n",
      ">7, 31/80, d1=0.35427, d2=0.32823 g=0.33440\n",
      ">7, 33/80, d1=0.32274, d2=0.33031 g=0.31509\n",
      ">7, 35/80, d1=0.33211, d2=0.34895 g=0.32690\n",
      ">7, 37/80, d1=0.33746, d2=0.31811 g=0.33657\n",
      ">7, 39/80, d1=0.31907, d2=0.33045 g=0.31806\n",
      ">7, 41/80, d1=0.32495, d2=0.32453 g=0.33903\n",
      ">7, 43/80, d1=0.32131, d2=0.33270 g=0.34174\n",
      ">7, 45/80, d1=0.34382, d2=0.29900 g=0.32583\n",
      ">7, 47/80, d1=0.32751, d2=0.33411 g=0.33616\n",
      ">7, 49/80, d1=0.34591, d2=0.33223 g=0.32184\n",
      ">7, 51/80, d1=0.34207, d2=0.34289 g=0.33446\n",
      ">7, 53/80, d1=0.32506, d2=0.31554 g=0.33730\n",
      ">7, 55/80, d1=0.33000, d2=0.32728 g=0.32436\n",
      ">7, 57/80, d1=0.32117, d2=0.34447 g=0.31707\n",
      ">7, 59/80, d1=0.32572, d2=0.31848 g=0.31562\n",
      ">7, 61/80, d1=0.33359, d2=0.30946 g=0.33000\n",
      ">7, 63/80, d1=0.33435, d2=0.33429 g=0.32457\n",
      ">7, 65/80, d1=0.31743, d2=0.31478 g=0.31420\n",
      ">7, 67/80, d1=0.34630, d2=0.33146 g=0.32220\n",
      ">7, 69/80, d1=0.31649, d2=0.32710 g=0.31948\n",
      ">7, 71/80, d1=0.31018, d2=0.32335 g=0.34469\n",
      ">7, 73/80, d1=0.34145, d2=0.33299 g=0.34242\n",
      ">7, 75/80, d1=0.33483, d2=0.31306 g=0.32521\n",
      ">7, 77/80, d1=0.33899, d2=0.31960 g=0.31262\n",
      ">7, 79/80, d1=0.32285, d2=0.32577 g=0.32819\n",
      ">8, 1/80, d1=0.34502, d2=0.33697 g=0.33048\n",
      ">8, 3/80, d1=0.34610, d2=0.32016 g=0.32996\n",
      ">8, 5/80, d1=0.31444, d2=0.33099 g=0.33097\n",
      ">8, 7/80, d1=0.32448, d2=0.33592 g=0.33364\n",
      ">8, 9/80, d1=0.32636, d2=0.34580 g=0.33472\n",
      ">8, 11/80, d1=0.33027, d2=0.31870 g=0.33853\n",
      ">8, 13/80, d1=0.33368, d2=0.32450 g=0.33876\n",
      ">8, 15/80, d1=0.31733, d2=0.32873 g=0.30929\n",
      ">8, 17/80, d1=0.33346, d2=0.32240 g=0.32785\n",
      ">8, 19/80, d1=0.33370, d2=0.33947 g=0.31740\n",
      ">8, 21/80, d1=0.29939, d2=0.31955 g=0.33580\n",
      ">8, 23/80, d1=0.32711, d2=0.30697 g=0.32754\n",
      ">8, 25/80, d1=0.29967, d2=0.32934 g=0.33752\n",
      ">8, 27/80, d1=0.32016, d2=0.33440 g=0.31799\n",
      ">8, 29/80, d1=0.33532, d2=0.34907 g=0.34711\n",
      ">8, 31/80, d1=0.34633, d2=0.32681 g=0.32216\n",
      ">8, 33/80, d1=0.33622, d2=0.33873 g=0.32437\n",
      ">8, 35/80, d1=0.31351, d2=0.30712 g=0.32540\n",
      ">8, 37/80, d1=0.32624, d2=0.33099 g=0.33045\n",
      ">8, 39/80, d1=0.33171, d2=0.33894 g=0.33171\n",
      ">8, 41/80, d1=0.31943, d2=0.30522 g=0.34094\n",
      ">8, 43/80, d1=0.31688, d2=0.31148 g=0.32312\n",
      ">8, 45/80, d1=0.33728, d2=0.31665 g=0.32437\n",
      ">8, 47/80, d1=0.32500, d2=0.31738 g=0.30641\n",
      ">8, 49/80, d1=0.31620, d2=0.32807 g=0.33592\n",
      ">8, 51/80, d1=0.32665, d2=0.33392 g=0.32671\n",
      ">8, 53/80, d1=0.33120, d2=0.33936 g=0.32712\n",
      ">8, 55/80, d1=0.32876, d2=0.34456 g=0.33761\n",
      ">8, 57/80, d1=0.31169, d2=0.32393 g=0.33736\n",
      ">8, 59/80, d1=0.32727, d2=0.31725 g=0.32781\n",
      ">8, 61/80, d1=0.33317, d2=0.32304 g=0.33322\n",
      ">8, 63/80, d1=0.32424, d2=0.33516 g=0.32015\n",
      ">8, 65/80, d1=0.31822, d2=0.31659 g=0.34116\n",
      ">8, 67/80, d1=0.33269, d2=0.31262 g=0.32365\n",
      ">8, 69/80, d1=0.33820, d2=0.33959 g=0.32110\n",
      ">8, 71/80, d1=0.33902, d2=0.32667 g=0.31912\n",
      ">8, 73/80, d1=0.32386, d2=0.34776 g=0.32108\n",
      ">8, 75/80, d1=0.32280, d2=0.33199 g=0.33434\n",
      ">8, 77/80, d1=0.31286, d2=0.33428 g=0.31937\n",
      ">8, 79/80, d1=0.32055, d2=0.33445 g=0.33271\n",
      ">9, 1/80, d1=0.33579, d2=0.32489 g=0.34094\n",
      ">9, 3/80, d1=0.35304, d2=0.33217 g=0.31974\n",
      ">9, 5/80, d1=0.33065, d2=0.33618 g=0.32615\n",
      ">9, 7/80, d1=0.30952, d2=0.32963 g=0.32050\n",
      ">9, 9/80, d1=0.32841, d2=0.33729 g=0.33301\n",
      ">9, 11/80, d1=0.33801, d2=0.32583 g=0.31397\n",
      ">9, 13/80, d1=0.32786, d2=0.32192 g=0.32865\n",
      ">9, 15/80, d1=0.32360, d2=0.33058 g=0.31528\n",
      ">9, 17/80, d1=0.33929, d2=0.31892 g=0.32821\n",
      ">9, 19/80, d1=0.33839, d2=0.33599 g=0.32672\n",
      ">9, 21/80, d1=0.31925, d2=0.33125 g=0.32955\n",
      ">9, 23/80, d1=0.31302, d2=0.32610 g=0.33458\n",
      ">9, 25/80, d1=0.33197, d2=0.34912 g=0.31343\n",
      ">9, 27/80, d1=0.33560, d2=0.31561 g=0.31791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">9, 29/80, d1=0.31511, d2=0.32372 g=0.31995\n",
      ">9, 31/80, d1=0.35853, d2=0.31663 g=0.35507\n",
      ">9, 33/80, d1=0.36206, d2=0.33029 g=0.33232\n",
      ">9, 35/80, d1=0.34699, d2=0.32185 g=0.32847\n",
      ">9, 37/80, d1=0.31940, d2=0.33363 g=0.33228\n",
      ">9, 39/80, d1=0.34833, d2=0.34070 g=0.32481\n",
      ">9, 41/80, d1=0.32289, d2=0.32321 g=0.31682\n",
      ">9, 43/80, d1=0.32901, d2=0.33077 g=0.32898\n",
      ">9, 45/80, d1=0.34533, d2=0.31973 g=0.33878\n",
      ">9, 47/80, d1=0.30873, d2=0.32587 g=0.32267\n",
      ">9, 49/80, d1=0.32499, d2=0.33153 g=0.32351\n",
      ">9, 51/80, d1=0.32165, d2=0.32721 g=0.34015\n",
      ">9, 53/80, d1=0.33457, d2=0.33234 g=0.33132\n",
      ">9, 55/80, d1=0.33954, d2=0.31173 g=0.33257\n",
      ">9, 57/80, d1=0.33376, d2=0.34945 g=0.32131\n",
      ">9, 59/80, d1=0.36489, d2=0.31595 g=0.34515\n",
      ">9, 61/80, d1=0.32577, d2=0.33321 g=0.33502\n",
      ">9, 63/80, d1=0.33122, d2=0.31861 g=0.31926\n",
      ">9, 65/80, d1=0.30932, d2=0.33048 g=0.31643\n",
      ">9, 67/80, d1=0.33406, d2=0.34401 g=0.31859\n",
      ">9, 69/80, d1=0.33880, d2=0.33832 g=0.33300\n",
      ">9, 71/80, d1=0.31941, d2=0.32629 g=0.32373\n",
      ">9, 73/80, d1=0.31495, d2=0.33429 g=0.32059\n",
      ">9, 75/80, d1=0.32191, d2=0.34344 g=0.33870\n",
      ">9, 77/80, d1=0.31911, d2=0.31408 g=0.32758\n",
      ">9, 79/80, d1=0.32171, d2=0.32379 g=0.31967\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 186\n",
    "# size of the data\n",
    "data = 186\n",
    "# classes\n",
    "classes = 3\n",
    "\n",
    "n_epochs=10\n",
    "\n",
    "# multiples of three (three classes) (less thyan 24000)\n",
    "n_batch=300\n",
    "\n",
    "# Loss Values\n",
    "G_L = np.infty\n",
    "\n",
    "# create the discriminator\n",
    "d_model = discriminator(data_dim=data)\n",
    "# d_model = discriminator(data_dim=data, input_classes=classes)\n",
    "\n",
    "# create the generator\n",
    "g_model = generator(noise_dim=data, beat_dim=data, out_dim=data)\n",
    "# g_model = generator(noise_dim=latent_dim, input_classes=classes, out_dim=data)\n",
    "\n",
    "# create the gan\n",
    "gan_model = create_gan(d_model, g_model)\n",
    "\n",
    "folder_name = 'Final/Beat_Input/'\n",
    "if not os.path.isdir(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "plot_model(d_model, to_file=folder_name+'disc.pdf', show_shapes=True)\n",
    "plot_model(g_model, to_file=folder_name+'gen.pdf', show_shapes=True)\n",
    "plot_model(gan_model, to_file=folder_name+'gan.pdf', show_shapes=True)\n",
    "\n",
    "\n",
    "# load image data\n",
    "X_N, y_N, X_S, y_S, X_V, y_V = load_real_samples()\n",
    "# # train model\n",
    "# train(g_model, d_model, gan_model, dataset, latent_dim)\n",
    "\n",
    "bat_per_epo = int(y_S.shape[0] / n_batch)\n",
    "half_batch = int(n_batch / 2)\n",
    "\n",
    "plt.ioff()\n",
    "    \n",
    "filename = folder_name + 'Plots'\n",
    "if not os.path.isdir(filename):\n",
    "    os.mkdir(filename)\n",
    "\n",
    "model_name = folder_name + 'Model/'\n",
    "if not os.path.isdir(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "f = open(folder_name + 'Loss.csv', 'w')\n",
    "f.write('d_loss1, d_loss2, g_loss \\n')\n",
    "f.close()\n",
    "\n",
    "f = open(folder_name + 'Stats.csv', 'w')\n",
    "for i in range(3):\n",
    "    for mtc in metric_to_calculate:\n",
    "        f.write(str(mtc)+'_'+str(i)+',')\n",
    "f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "# manually enumerate epochs\n",
    "for i in range(n_epochs):\n",
    "    # enumerate batches over the training set\n",
    "    for j in range(bat_per_epo):\n",
    "        \n",
    "        # get randomly selected 'real' samples\n",
    "        [X_real, labels_real], y_real = generate_real_samples(X_N, y_N, X_S, y_S, X_V, y_V, half_batch)\n",
    "        # print (X_real.shape, labels_real.shape, y_real.shape)\n",
    "        # update discriminator model weights\n",
    "        d_loss1, _ = d_model.train_on_batch([X_real, X_real], y_real)\n",
    "        \n",
    "        # generate 'fake' examples\n",
    "        [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # print (X_fake.shape, labels.shape, y_fake.shape)\n",
    "        # update discriminator model weights\n",
    "        d_loss2, _ = d_model.train_on_batch([X_fake, X_fake], y_fake)\n",
    "        \n",
    "        # prepare points in latent space as input for the generator\n",
    "        [z_input, _] = generate_latent_points(latent_dim, n_batch)\n",
    "        [X, _], _ = generate_real_samples(X_N, y_N, X_S, y_S, X_V, y_V, n_batch)\n",
    "        # print (z_input.shape)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = reshape(np.random.uniform(0.8, 1, n_batch))\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss = gan_model.train_on_batch([z_input, X], y_gan)\n",
    "        \n",
    "#         if g_loss < G_L:\n",
    "#             G_L = g_loss\n",
    "#             g_model.save(model_name + str(i*1000 + j) + '_cgan_generator.h5')\n",
    "\n",
    "        f = open(folder_name + 'Loss.csv', 'a')\n",
    "        f.write(str(d_loss1)+','+str(d_loss2)+','+str(g_loss)+'\\n')\n",
    "        f.close()\n",
    "\n",
    "        if (j+1)%2 == 0:\n",
    "            print('>%d, %d/%d, d1=%.5f, d2=%.5f g=%.5f' %(i, j, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "            name = filename+'/'+str(i*1000 + j)+'.jpg'\n",
    "            # generate ECGs\n",
    "            z_input  = g_model.predict([z_input, X])\n",
    "            X_R = get_real_samples(X_N, y_N, X_S, y_S, X_V, y_V)\n",
    "            save_new_plot(X_R, z_input, n_batch, name)\n",
    "            g_model.save(model_name + str(i*1000 + j) + '_cgan_generator.h5')\n",
    "            \n",
    "        if (j+1)%10 == 0:\n",
    "            evaluate(X, z_input, classes, metric_to_calculate, n_batch, folder_name, samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-welsh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
